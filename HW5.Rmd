---
title: "R Notebook"
output:
  pdf_document: default
  html_notebook: default
---


This question involves the use of multiple linear regression on the Auto data set from the
course webpage (https://scads.eecs.wsu.edu/index.php/datasets/). Ensure that you remove
missing values from the dataframe, and that values are represented in the appropriate types.

```{r}
library(tidyverse)
library(ggplot2)
library(dplyr)
Auto <- read.csv("Auto.csv")
```

a. (5%) Perform a multiple linear regression with mpg as the response and all other
variables except name as the predictors. Show a printout of the result (including
coefficient, error and t values for each predictor). Comment on the output:
i) Which predictors appear to have a statistically significant relationship to the response,
and how do you determine this?


```{r}
part_a <- lm(data = Auto[,1:8], mpg ~.)
Sm <- summary(part_a)
Sm$coefficients

```
If we look at the horsepower in the result, 
The relationship above, suggests that displacement and year (of make) have a statistical relationship with MPG. We can determine this by examining the regression coefficient (both the magnitude and polarity)


ii) What does the coefficient for the displacement variable suggest, in simple terms?

As given above, the coefficient for the displacement is 0.002486107(positive) which determines that as the MPG increases, so has the displacement. 


b. (5%) Produce diagnostic plots of the linear regression fit. Comment on any problems you
see with the fit. Do the residual plots suggest any unusually large outliers? Does the
leverage plot identify any observations with unusually high leverage?

```{r}
par(mfrow=c(2,2)) 
part_b <- plot(part_a)
```


c. (5%) Fit linear regression models with interaction effects. Do any interactions appear to
be statistically significant?

```{r}
part_c = lm(mpg ~ . - name + weight:acceleration,data=Auto)
summary(part_c)

```
This problem involves the Boston data set, which we saw in class. We will now try to predict
per capita crime rate using the other variables in this data set. In other words, per capita
crime rate is the response, and the other variables are the predictors.
```{r}
library(reticulate)
library(MASS)
head(Boston)
```

a. (6%) For each predictor, fit a simple linear regression model to predict the response.
Include the code, but not the output for all models in your solution.
```{r}
fit.age <- lm(crim ~ age, data = Boston)
summary(fit.age)
```

```{r}
fit.rm <- lm(crim ~ rm, data = Boston )
summary(fit.rm)
```

c. (6%) Fit a multiple regression model to predict the response using all the predictors.
Describe your results. For which predictors can we reject the null hypothesis H0 : βj = 0?
```{r}
q2_partc <- lm(crim ~ . - crim , data = Boston )
summary(q2_partc)
```
We can determine that very less number of variables actually are significant enough. If we look at the "Pr(>|t|)" column, we see that dis and rad are both are at 0.000502 and 6.46e-11 >= 0.5.There are also denoted significant with (***) on the signuf codes. 
All the other variables will be invlove rejecting the null hypothesis. 
 
 
d. (6%) How do your results from (a) compare to your results from (c)? Create a plot
displaying the univariate regression coefficients from (a) on the x-axis, and the multiple
regression coefficients from (c) on the y-axis. That is, each predictor is displayed as a
single point in the plot. Its coefficient in a simple linear regression model is shown on the 
2
x-axis, and its coefficient estimate in the multiple linear regression model is shown on the
y-axis. What does this plot tell you about the various predictors?

```{r}
case1 <- lm(crim ~ zn, data = Boston)$coefficients[2] 
case1 <- append(case1, lm(crim ~ indus, data = Boston)$coefficients[2])
case1 <- append(case1, lm(crim ~ chas, data = Boston)$coefficients[2])
case1 <- append(case1, lm(crim ~ nox, data = Boston)$coefficients[2])
case1 <- append(case1, lm(crim ~ rm, data = Boston)$coefficients[2])
case1 <- append(case1, lm(crim ~ age, data = Boston)$coefficients[2])
case1 <- append(case1, lm(crim ~ dis, data = Boston)$coefficients[2])
case1 <- append(case1, lm(crim ~ rad, data = Boston)$coefficients[2])
case1 <- append(case1, lm(crim ~ tax, data = Boston)$coefficients[2])
case1 <- append(case1, lm(crim ~ ptratio, data = Boston)$coefficients[2])
case1 <- append(case1, lm(crim ~ black, data = Boston)$coefficients[2])
case1 <- append(case1, lm(crim ~ lstat, data = Boston)$coefficients[2])
case1 <- append(case1, lm(crim ~ medv, data = Boston)$coefficients[2])
case2 <- (lm(crim ~ . - crim, data = Boston))
case2$coefficients[2:14]
```

```{r}
plot(case1, case2$coefficients[2:14], main = "Univariate vs. Multiple", 
    xlab = "Univariate Regression Coefficient", ylab = "Multiple Regression Coefficients")
```


e. (6%) Is there evidence of non-linear association between any of the predictors and the
response? To answer this question, for each predictor X, fit a model of the form
Y = β0 + β1X + β2X
2 + β3X
3+ ε
Hint: use the poly() function in R. Again, include the code, but not the output for
each model in your solution, and instead describe any non-linear trends you
uncover. 


Q3)Suppose we collect data for a group of students in a statistics class with variables:
X1 = hours studied,
X2 = undergrad GPA,
X3 = PSQI score (a sleep quality index), and
Y = receive an A.
We fit a logistic regression and produce estimated coefficient, β0 = −7, β1 = 0.1, β2 = 1, β3 = -.04.
a. (5%) Estimate the probability that a student who studies for 32 h, has a PSQI score of 12
and has an undergrad GPA of 3.0 gets an A in the class. Show your work.

Answer: p(x) = exp(-7+(0.1*32)+(1*12)+(-.04*3))/(1+exp(-7+(0.1*32)+(1*12)+(-0.04*3)))
x1 = 32
x2 = 3.0
x3 = 12
B0 = -7
B1 = 0.1
B2 = 1
B3 = -.04
y = (B0 + (b1*x1) + (b2*x2) + (b3*x3)) = 1.279999
ln(1.2799) = 0.24
             
b. (5%) How many hours would the student in part (a) need to study to have a 50 % chance
of getting an A in the class? Show your work.

We are given that p(x) = 0.5 as the (50% chance of getting an A)
x1 = ?
x2 = 3.0
x3 = 12
      0.50 = -7 + (0.1*x) + (3*1) + (-.04*12) = 40.2hr
      
c. (5%) How many hours would a student with a 3.0 GPA and a PSQI score of 3 need to
study to have a 50 % chance of getting an A in the class? Show your work.

x1 = ?
x2 = 3.0
x3 = 3 
    0.50 = -7 + (0.1 * x1) + (1 * 3.0) + (-0.4 * 3) = 46.2hr
  
  
Q4) For this question, you will a naïve Bayes model to classify newspaper articles by their
section. You will be provided a set of news articles

```{r}
library(readr)
Articles <- read.csv("GuardianArticles.csv")
```

a. Tokenization (20%)
In order to use Naïve Bayes effectively, you will need to split your text into tokens. It is
common practice when doing this to reduce your words to their stems so that
conjugations produce less noise in your data. For example, the words "speak", "spoke",
and "speaking" are all likely to denote a similar context, and so a stemmed tokenization
will merge all of them into a single stem. R has several libraries for tokenization,
stemming and text mining. Some you may want to use as a starting point are tokenizers,
SnowballC, tm respectively, or alternatively quanteda, which will handle the
aforementioned along with building your model in the next step. You will need to
produce a document-term matrix from your stemmed tokenized data. This will have a
very wide feature set (to be reduced in the following step) where each word stem is a
feature, and each article has a list of values representing the number of occurrences of
each stem in its body. Before representing the feature set in a non-compact storage format 
(such as a plain matrix), you will want to remove any word which appears in too few
documents (typically fewer than 1% of documents, but you can be more or less stringent
as you see fit). You may also use a boolean for word presence/absence if you find it more
effective. To demonstrate your completion of this part, you can simply select and print
the text of a random article along with the non-zero entries of its feature vector.

```{r}
library(tidytext)
Articles
```

b. Classification (20%)
For the final portion of this assignment, you will build and test a Naïve Bayes classifier
with your data. First, you will need to use feature selection to reduce your feature set. A
popular library for this is caret. It has many functionalities for reducing feature sets,
including removing highly correlated features. You may wish to try several different
methods to see which produces the best results for the following steps.
Next, you will split your data into a training set and a test set. Your training set should
comprise approximately 80% of your articles, however, you may try several sizes to find
which produces the best results. Whatever way you split your training and test sets,
however, you should try to ensure that your six article categories are equally represented
in both sets.
Next, you will build your Naïve Bayes classifier from your training data. The e1071
package is most commonly used for this. Finally, you can use your model to predict the
categories of your test data.
Once you have produced a model that generates the best predictions you can get, print a
confusion matrix of the results to demonstrate your completion of this task. For each
class, give scores for precision (TruePositives / TruePositives+FalsePositives) and recall
(TruePositives / TruePositives+FalseNegatives). To do this, you may want to use the
confusionMatrix() function. 